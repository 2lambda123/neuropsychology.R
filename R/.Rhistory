df <- data.frame(pop=population)
plot <- ggplot(df, aes(pop)) +
geom_density(fill = "#2196F3", colour="white", adjust = 3, na.rm=TRUE) +
geom_vline(xintercept=score,size = 2, color="#E91E63") +
xlab("\nScore") +
ylab("Distribution\n") +
theme_neuropsychology()
# theme_bw() +
# scale_fill_brewer(palette="Set1") +
# scale_colour_brewer(palette="Set1",direction=1) +
# ggsave("C_DERS_strategie.png", width = 14, height = 7,units = "cm")
percentile <- ecdf(population)
percentile <- percentile(score)
if(percentile<0.50){
percentile <- 1-percentile
comparison <- "smaller"
}else{comparison <- "greater"}
if(language=="fr"){
if(comparison=="smaller"){
comparison <- "inférieur"
}else{comparison <- "supérieur"}
print(paste("Le participant (score = ",
score,
") se situe a ",
as.character(round((score-mean(population))/sd(population), 2)),
" écarts-types de la moyenne (M = ",
as.character(round(mean(population),2)),
", écart-type = ",
as.character(round(sd(population),2)),
")", sep=""))
print(paste("Le score du participant est",
comparison,
"à",
as.character((round(percentile*100, 2))),
"% de la population générale."))
}else{
print(paste("The participant (score = ",
score,
") stands at ",
as.character(round((score-mean(population))/sd(population), 2)),
" standart deviations from the mean (M = ",
as.character(round(mean(population),2)),
", SD = ",
as.character(round(sd(population),2)),
")", sep=""))
print(paste("The participant's score is",
comparison,
"than",
as.character((round(percentile*100, 2))),
"% of the general population."))
}
return(plot)
}
assess(1.2)
assess(-1.2)
install.packages("devtools")
library("devtools")
install_github("neuropsychology/neuropsychology.R")
library("neuropsychology")
cortable(personality)
cortable <- function(df,
correction="holm",
type="pearson",
returns="table",
print.result=TRUE,
plot.result=TRUE,
iamaboringperson=FALSE){
type <- ifelse(type == "s", "spearman",
ifelse(type == "spearman", "spearman","pearson"))
correction_text <- ifelse(correction=="holm", "Holm-Bonferroni",
ifelse(correction=="fdr", "False Discovery Rate",correction))
if(length(names(df)) > 15 && correction=="none" && iamaboringperson==FALSE){
warning("We've detected that you are running a lot (> 15) of correlation tests without adjusting the p values.
To help you in your p fishing, we've added some cool variables:
You never know, you might find something significant and get to publish this paper!
\nTo deactivate this, change the 'iamaboringperson' argument to TRUE.
Cheers.")
df$Local_Air_Density <- rnorm(nrow(df), mean=47, sd=12)
df$Number_of_Reproducing_Frogs <- runif(nrow(df), max=100)
df$Researchers_Hapinness <- rnorm(nrow(df), mean=0, sd=1)
df$Aliens_Motherships_Distance <- rnorm(nrow(df), mean=50000, sd=5000)
df$Gods_Desctrutive_Power <- runif(nrow(df), max=10)
}
for (i in names(df)){
if (is.numeric(df[,i]) == FALSE){
df[,i] = NULL
}
}
dimnames <- names(df)
df <- as.matrix(df)
R <- rcorr(df, type = type)$r
p <- rcorr(df, type = type)$P
p <- p.adjust(p, method = correction)
p.mat <- matrix(p, ncol = ncol(R), dimnames = list(dimnames,dimnames))
## define notions for significance levels; spacing is important.
mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "* ", " ")))
## trunctuate the matrix that holds the correlations to two decimal
R <- format(round(cbind(rep(-1.11, ncol(df)), R), 2))[,-1]
## build a new matrix that includes the correlations with their apropriate stars
table <- matrix(paste(R, mystars, sep=""), ncol=ncol(df))
diag(table) <- paste(diag(R), " ", sep="")
rownames(table) <- colnames(df)
colnames(table) <- paste(colnames(df), "", sep="")
## remove upper triangle
table <- as.matrix(table)
table[upper.tri(table, diag = TRUE)] <- NA
table <- as.data.frame(table)
## remove last column and return the matrix (which is now a data frame)
table <- cbind(table[1:length(table)-1])
if (print.result==TRUE){
print(paste("A ", type, "'s correlation matrix (correction: ", correction_text, ")", sep = ""))
print(table)
}
plot <- ggcorrplot(rcorr(df, type = type)$r,
title = paste("A ", type, "'s correlation matrix (correction: ", correction_text, ")\n", sep = ""),
method = "circle",
type="lower",
colors=c("#E91E63", "white", "#03A9F4"),
hc.order = TRUE,
p.mat = p.mat,
insig="pch",
legend.title="",
lab=FALSE) +
theme(plot.title = element_text(hjust = 0.7))
if (plot.result==TRUE){
print(plot)
}
if (returns=="table"){
return(table)
}
else{
return(plot)
}
}
cortable(personality)
cortable(personality)
cortable(personality, correction="none")
library(wordcloud2)
?wordcloud2
a <- 1:20
a <- as.vector(a)
n_colours <- function(colours="all"){
Ncolours_Reds <- c("#f44336", "#E91E63", "#9C27B0","#673AB7")
Ncolours_Blues <- c("#3F51B5","#2196F3","#03A9F4","#00BCD4")
Ncolours_Greens <- c("#009688", "#4CAF50", "#8BC34A", "#CDDC39")
Ncolours_Yellows <- c("#FFEB3B", "#FFC107", "#FF9800","#FF5722")
Ncolours_Greys <- c("#795548", "#9E9E9E", "#607D8B")
Ncolours_All <- c(Ncolours_Reds,
Ncolours_Blues,
Ncolours_Greens,
Ncolours_Yellows,
Ncolours_Greys)
if(tolower(colours)=="all"){
return(Ncolours_All)
}
else if(tolower(colours)=="reds"){
return(Ncolours_Reds)
}
else if(tolower(colours)=="blues"){
return(Ncolours_Blues)
}
else if(tolower(colours)=="greens"){
return(Ncolours_Greens)
}
else if(tolower(colours)=="yellows"){
return(Ncolours_Yellows)
}
else if(tolower(colours)=="greys"){
return(Ncolours_Greys)
}
else{
print(paste(colours, "unknown. Please check documentation."))
}
}
c <- n_colours()
a <- 1:67
d <- replicate(c, length(a))
?replicate
?replicate
?rep
d <- rep(c, length(a))
d <- rep(c, length.out= length(a))
?wordcloud2
is.list(c(1,2,3))
is.vector(c(1,2,3))
value <- 7
intepretation <- ifelse(value < 2, "Not worth more than a bare mention",
ifelse(p < 6, "Positive",
ifelse(p < 10, "Strong",
"Very Strong")))
intepretation <- ifelse(value < 2, "Not worth more than a bare mention",
ifelse(value < 6, "Positive",
ifelse(value < 10, "Strong",
"Very Strong")))
print(interpretation)
print(intepretation)
print(paste("The evidence in favor of H1 is", intepretation))
print(source)
source <- "Raftery, A. E. 1995. Bayesian model selection in social research. In Vol. 25 of Sociological Methodology, ed. P. V.
Marsden, 111–163. Oxford: Blackwell."
print(source)
interpret <- function(value, what="BIC"){
if(what=="BIC"){
intepretation <- ifelse(value < 2, "not worth more than a bare mention",
ifelse(value < 6, "Positive",
ifelse(value < 10, "strong",
"very Strong")))
source <- "Raftery, A. E. 1995. Bayesian model selection in social research. In Vol. 25 of Sociological Methodology, ed. P. V. Marsden, 111–163. Oxford: Blackwell."
}
if(what=="AIC"){
intepretation <- ifelse(value < 2, "substantial",
ifelse(value < 4, "moderate",
ifelse(value < 7, "little",
"essentially null")))
source <- "Burnham, K. P., & Anderson, D. R. (2002). Information and likelihood theory: a basis for model selection and inference. Model selection and multimodel inference: a practical information-theoretic approach, 49-97."
}
print(paste("The evidence in favor of H1 can be considered as", intepretation))
print(source)
return(intepretation)
}
interpret(4.5, "AIC")
all <- list("interpretation"=interpretation, "source"=source)
interpretation=3
source="el"
all <- list("interpretation"=interpretation, "source"=source)
warnings("The following result is based on the method described in Makowski et al. (2017).")
message("The following result is based on the method described in Makowski et al. (2017).")
message("Warning: The following result is based on the method described in Makowski et al. (2017), which present severe limitations. Use for exploration only.")
odds_to_probs <- function(odds, log=TRUE){
if (log==TRUE){
odds = exp(odds)
}
probs = odds/(1+odds)
return(probs)
}
odds_to_probs(0.46)
library(installr)
updateR()
library(neuropsychology)
df <- personality
df <- select_numeric(df)
n_total <- nrow(df)
variables <- colnames(df)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=variables,
missings=n_total - n,
prop_missings=missings/n_total,
mean=mean,
sd=sd,
median=median,
min=min,
max=max,
skew=skew,
kurtosis=kurtosis)
df %>%
psych::describe() %>%
dplyr::transmute_(variable="variables",
missings="n_total" - "n",
prop_missings=missings/n_total,
mean=mean,
sd=sd,
median=median,
min=min,
max=max,
skew=skew,
kurtosis=kurtosis)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=lazyeval::interp("variables"),
missings=lazyeval::interp("n_total - n"),
prop_missings=missings/n_total,
mean=mean,
sd=sd,
median=median,
min=min,
max=max,
skew=skew,
kurtosis=kurtosis)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=lazyeval::interp("variables"),
missings=lazyeval::interp("n_total - n"),
prop_missings=lazyeval::interp("missings/n_total"),
mean=mean,
sd=sd,
median=median,
min=min,
max=max,
skew=skew,
kurtosis=kurtosis)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=lazyeval::interp("variables"),
missings=lazyeval::interp("n_total - n"),
prop_missings=lazyeval::interp("missings/n_total"),
mean=lazyeval::interp("mean"),
sd=lazyeval::interp("sd"),
median=lazyeval::interp("median"),
min=lazyeval::interp("min"),
max=lazyeval::interp("max"),
skew=lazyeval::interp("skew"),
kurtosis=lazyeval::interp("kurtosis"))
df %>%
psych::describe() %>%
dplyr::transmute(variable=lazyeval::interp("variables"),
missings=lazyeval::interp("n_total - n"),
prop_missings=lazyeval::interp("missings/n_total"),
mean=lazyeval::interp("mean"),
sd=lazyeval::interp("sd"),
median=lazyeval::interp("median"),
min=lazyeval::interp("min"),
max=lazyeval::interp("max"),
skew=lazyeval::interp("skew"),
kurtosis=lazyeval::interp("kurtosis"))
df %>%
psych::describe() %>%
dplyr::transmute_(variable=variables,
missings=n_total - n,
prop_missings=missings/n_total,
mean=mean,
sd=sd,
median=median,
min=min,
max=max,
skew=skew,
kurtosis=kurtosis)
df %>%
psych::describe()
df %>%
psych::describe() %>%
dplyr::transmute_(variable=~variables,
missings=~n_total-~n,
prop_missings=~missings/~n_total,
mean=~mean,
sd=~sd,
median=m~edian,
min=~min,
max=~max,
skew=~skew,
kurtosis=~kurtosis)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=~variables,
missings=~n_total-~n,
prop_missings=~missings/~n_total,
mean=~mean,
sd=~sd,
median=~median,
min=~min,
max=~max,
skew=~skew,
kurtosis=~kurtosis)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=quote(variables),
missings=quote(n_total)-quote(n),
prop_missings=quote(missings)/quote(n_total),
mean=quote(mean),
sd=quote(sd),
median=quote(median),
min=quote(min),
max=quote(max),
skew=quote(skew),
kurtosis=quote(kurtosis))
df %>%
psych::describe() %>%
dplyr::transmute_(variable=quote(variables))
df %>%
psych::describe() %>%
dplyr::transmute_(variable="variables")
df %>%
psych::describe() %>%
dplyr::transmute_(variable=~variables)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=~variables,
missings=~n_total-n)
df %>%
psych::describe() %>%
dplyr::transmute_(variable=~variables,
missings=~n_total-n,
prop_missings=~missings/n_total,
mean=~mean,
sd=~sd,
median=~median,
min=~min,
max=~max,
skew=~skew,
kurtosis=~kurtosis)
fa <- fa(select(df, starts_with("ASQ")))
library(psych)
fa <- psych::fa(select(df, starts_with("FFMQ")))
library(psych)
fa <- psych::fa(select(personality, starts_with("FFMQ")))
df <- df_original %>%
filter(!is.na(Detection_Answer),
!is.na(Recognition_Answser),
Blind==F) %>%
mutate(Detection = ifelse(Detection_Answer=="Detected", 1, 0),
Detection=as.factor(Detection),
Full_Detection = Target_Category_Correct,
Recognition_Answer=Recognition_Answser,
Recognition=as.factor(ifelse(Recognition_Answer=="Non", 0, 1))) %>%
select(Participant_ID,
Stimulus,
Condition,
SOA,
SOA_Mean_Inversion,
Recognition,
Recognition_Answer,
Detection_Answer,
Detection,
Full_Detection,
starts_with("FFMQ_"),
starts_with("ASQ_"))
fa <- psych::fa(select(personality, starts_with("FFMQ")))
df <- select(personality, starts_with("FFMQ"))
df <- select(personality, starts_with("ASQ"))
df <- select(personality, starts_with("IPIP"))
summary(personality)
df <- select_numeric(personality)
fa <- psych::fa(df)
treshold=0
round=2
labels=NA
loadings <- fa$loadings %>%
dplyr::unclass() %>%
as.data.frame()
loadings[abs(loadings) <= abs(treshold)] <- NA
loadings <- fa$loadings %>%
unclass() %>%
as.data.frame()
loadings[abs(loadings) <= abs(treshold)] <- NA
n_factors <- length(loadings)
loadings$Item <- rownames(loadings)
if(length(labels)==nrow(loadings)){loadings$Label <- labels}
else{loadings$Label <- 1:nrow(loadings)}
# Add item labels
loadings$Item <- rownames(loadings)
if(length(labels)==nrow(loadings)){loadings$Label <- labels}
else{loadings$Label <- 1:nrow(loadings)}
loadings$Item <- rownames(loadings)
if(length(labels)==nrow(loadings))
{loadings$Label <- labels}
else
{loadings$Label <- 1:nrow(loadings)}
loadings$Item <- rownames(loadings)
if(length(labels)==nrow(loadings)){
} else {loadings$Label <- 1:nrow(loadings)}
loadings$N <- 1:nrow(loadings)
loadings %>%
dplyr::gather_(~Component, ~Loading, -~Item, -~N, -~Label) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(which.max(~Loading)) %>%
dplyr::arrange_(~Component, desc(~Loading))
loadings %>%
tidyr::gather_(~Component, ~Loading, -~Item, -~N, -~Label) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(which.max(~Loading)) %>%
dplyr::arrange_(~Component, desc(~Loading))
loadings %>%
tidyr::gather_(~Component, ~Loading, ~-Item, ~-N, ~-Label) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(which.max(~Loading)) %>%
dplyr::arrange_(~Component, desc(~Loading))
loadings %>%
tidyr::gather_(~Component, ~Loading, ~-Item, ~-N, ~-Label)
loadings %>%
tidyr::gather_(~Component, Loading, -Item, -N, -Label)
loadings %>%
tidyr::gather_("Component, Loading, -Item, -N, -Label") %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(which.max(~Loading)) %>%
dplyr::arrange_(~Component, desc(~Loading))
loadings %>%
tidyr::gather_("Component", "Loading", "-Item", "-N", "-Label")
loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label"))
loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label")))
loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label"))) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(which.max(~Loading)) %>%
dplyr::arrange_(~Component, desc(~Loading))
loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label"))) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(~which.max(Loading)) %>%
dplyr::arrange_(~Component, desc(~Loading))
loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label"))) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(~which.max(Loading)) %>%
dplyr::arrange_(~Component, desc(Loading))
loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label"))) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(~which.max(Loading)) %>%
dplyr::arrange_(~Component, ~desc(Loading))
max <- loadings %>%
tidyr::gather_("Component", "Loading", setdiff(names(.), c("Item", "N", "Label"))) %>%
dplyr::group_by_(~Item) %>%
dplyr::slice_(~which.max(Loading)) %>%
dplyr::arrange_(~Component, ~desc(Loading))
if(length(labels)!=nrow(loadings)){
max <- dplyr::select(max, -Label)
loadings <- dplyr::select(loadings, -Label)
}
loadings <- loadings[order(max$N), ]
loadings[1:n_factors] <- round(loadings[1:n_factors], round)
